{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30616,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "V100"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/djwackey/chinese-hiphop-lyrics.git\n",
        "import os,json\n",
        "\n",
        "allhiphop = os.walk('./chinese-hiphop-lyrics/')\n",
        "songs = []\n",
        "for root_dir,_,hiphops in allhiphop:\n",
        "    for hiphop in hiphops:\n",
        "       if hiphop.endswith(\".json\"):\n",
        "        song_path = f'{root_dir}/{hiphop}'\n",
        "        with open(song_path, 'r', encoding='utf-8') as file:\n",
        "                data = json.load(file)\n",
        "                for song in data['songs']:\n",
        "                    songs.append(song)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:46:51.411824Z",
          "iopub.execute_input": "2023-12-11T17:46:51.412225Z",
          "iopub.status.idle": "2023-12-11T17:46:52.603921Z",
          "shell.execute_reply.started": "2023-12-11T17:46:51.412193Z",
          "shell.execute_reply": "2023-12-11T17:46:52.602478Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJgj_a82BPSq",
        "outputId": "d71b62d9-4114-489b-e042-9f030dcb858b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'chinese-hiphop-lyrics' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "file_path = 'word.txt'\n",
        "for song in songs:\n",
        "    # print(song)\n",
        "    text = song['lyrics']\n",
        "    lines = 'EOS'.join(text)\n",
        "    with open(file_path, 'a', encoding='utf-8') as file:\n",
        "        file.write(lines)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:46:53.571515Z",
          "iopub.execute_input": "2023-12-11T17:46:53.571919Z",
          "iopub.status.idle": "2023-12-11T17:46:53.681885Z",
          "shell.execute_reply.started": "2023-12-11T17:46:53.571885Z",
          "shell.execute_reply": "2023-12-11T17:46:53.681096Z"
        },
        "trusted": true,
        "id": "8nVkc9I-BPSs"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Markov Chain Model"
      ],
      "metadata": {
        "id": "qzQeC2CWBPSt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import jieba  # Used for Chinese word segmentation\n",
        "\n",
        "def read_file(file_path):\n",
        "    # Read and return text from the file\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Segment the text into words using jieba and return the list of words\n",
        "    words = list(jieba.cut(text))\n",
        "    return words\n",
        "\n",
        "def build_markov_chain(words):\n",
        "    # Build a Markov chain dictionary where each word points to a list of words that come after it\n",
        "    markov_chain = {}\n",
        "    for i in range(len(words) - 1):\n",
        "        word = words[i]\n",
        "        next_word = words[i + 1]\n",
        "        if word not in markov_chain:\n",
        "            markov_chain[word] = []\n",
        "        markov_chain[word].append(next_word)\n",
        "    return markov_chain\n",
        "\n",
        "def generate_text(chain, length=20):\n",
        "    # Generate a text of specified length using the Markov chain\n",
        "    word1 = random.choice(list(chain.keys()))\n",
        "    result = [word1]\n",
        "    for _ in range(length - 1):\n",
        "        word2 = random.choice(chain[word1])\n",
        "        result.append(word2)\n",
        "        word1 = word2\n",
        "    return ''.join(result)\n",
        "\n",
        "file_path = './word.txt'  # Replace with your file path\n",
        "text = read_file(file_path)  # Read the text from the file\n",
        "words = preprocess_text(text)  # Preprocess the text for Markov chain\n",
        "chain = build_markov_chain(words)  # Build the Markov chain\n",
        "generated_text = generate_text(chain, 50)  # Generate text of length 50\n",
        "print(generated_text)  # Print the generated text"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:46:57.918496Z",
          "iopub.execute_input": "2023-12-11T17:46:57.918891Z",
          "iopub.status.idle": "2023-12-11T17:47:33.294461Z",
          "shell.execute_reply.started": "2023-12-11T17:46:57.918859Z",
          "shell.execute_reply": "2023-12-11T17:47:33.293436Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOfOaacNBPSt",
        "outputId": "e110958b-f9a4-43b9-f65e-6b9f92d97f7b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Building prefix dict from the default dictionary ...\n",
            "DEBUG:jieba:Building prefix dict from the default dictionary ...\n",
            "Loading model from cache /tmp/jieba.cache\n",
            "DEBUG:jieba:Loading model from cache /tmp/jieba.cache\n",
            "Loading model cost 0.805 seconds.\n",
            "DEBUG:jieba:Loading model cost 0.805 seconds.\n",
            "Prefix dict has been built successfully.\n",
            "DEBUG:jieba:Prefix dict has been built successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "westside\n",
            "超脱后悔过过招像日本出品嘿 世世你的路EOS在梦EOS加入我们都为她吃\n",
            "以前我制造 导步EOS人生被雪覆盖整片海域因为我的路EOS谁会跌倒的 can \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4L0rFx8bV8Vo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Read text data\n",
        "file_path = 'word.txt'\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Tokenize using jieba\n",
        "words = list(jieba.cut(text))\n",
        "\n",
        "# Build vocabulary\n",
        "vocab = set(words)\n",
        "word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "index_to_word = {idx: word for idx, word in enumerate(vocab)}\n",
        "\n",
        "# Convert text to index sequences\n",
        "sequences = [word_to_index[word] for word in words]\n",
        "\n",
        "# Create sequences and labels\n",
        "sequence_length = 10  # Set the length of each sequence\n",
        "sequences_input = []\n",
        "labels = []\n",
        "\n",
        "for i in range(len(sequences) - sequence_length):\n",
        "    seq = sequences[i:i + sequence_length]\n",
        "    label = sequences[i + sequence_length]\n",
        "    sequences_input.append(seq)\n",
        "    labels.append(label)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "sequences_input = torch.tensor(sequences_input, dtype=torch.long)\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n",
        "\n",
        "# Split into training and testing sets\n",
        "split_ratio = 0.8  # 80% for training, 20% for testing\n",
        "split_index = int(len(sequences_input) * split_ratio)\n",
        "\n",
        "train_sequences = sequences_input[:split_index]\n",
        "train_labels = labels[:split_index]\n",
        "\n",
        "test_sequences = sequences_input[split_index:]\n",
        "test_labels = labels[split_index:]\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 2048\n",
        "train_dataset = TensorDataset(train_sequences, train_labels)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_sequences, test_labels)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:47:33.296491Z",
          "iopub.execute_input": "2023-12-11T17:47:33.297149Z",
          "iopub.status.idle": "2023-12-11T17:47:33.303678Z",
          "shell.execute_reply.started": "2023-12-11T17:47:33.297112Z",
          "shell.execute_reply": "2023-12-11T17:47:33.302764Z"
        },
        "trusted": true,
        "id": "0JLQAvZtBPSu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7SrPqSqceJbL",
        "outputId": "ce076a15-5a1a-4e1c-b5e2-59458d01ede1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2616"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 查看训练数据和测试数据的一个批次\n",
        "for batch_inputs, batch_labels in train_data_loader:\n",
        "    # print(\"Train Batch Inputs:\", batch_inputs)\n",
        "    print(\"Train Batch Labels:\", batch_labels)\n",
        "    break\n",
        "\n",
        "for batch_inputs, batch_labels in test_data_loader:\n",
        "    # print(\"Test Batch Inputs:\", batch_inputs)\n",
        "    print(\"Test Batch Labels:\", batch_labels)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fKoPI8mLNJA",
        "outputId": "f38d49e8-083c-4864-fb52-a62dcb94282d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Batch Labels: tensor([ 9867, 12631, 11812, 35445, 40129, 37345, 20891, 36478, 11976, 37050,\n",
            "        15001, 72604, 37882, 11976, 37882, 65587, 11976, 76178, 32887, 58246,\n",
            "        18688, 79590, 67757, 11976, 47600, 57095, 72697, 61719, 70323, 36247,\n",
            "        43138, 42512, 54293,  4955, 35472, 37882, 72281, 32172, 45145, 23669,\n",
            "        34088, 23066, 64239, 11976, 42299, 12631, 11976, 40428, 26270, 11976,\n",
            "        11976, 62139, 39322, 67032,  3584, 42668, 42967, 21727, 21951, 47010,\n",
            "        38576, 32019,  5731, 23890])\n",
            "Test Batch Labels: tensor([32956, 11976, 64798, 11976, 59542, 11976, 32956, 11976, 31082, 11976,\n",
            "        38242, 11976, 32956, 11976, 49725, 45333, 17764, 22665, 11976, 24118,\n",
            "        11976, 20865, 11976, 43846, 17764, 12631, 22463, 72697,  3319, 71515,\n",
            "        17764, 12631, 46343, 72697,  3319, 71515, 17764, 13011, 23433, 43696,\n",
            "        72697,  3319, 71515, 17764, 68958, 67032,  2326, 15045, 11976, 67963,\n",
            "        28434, 35703, 17764, 74316, 70005, 54693, 65863, 11976,  6434, 30615,\n",
            "         4955, 67032, 34128, 17764])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assume you already have processed data: 'train_sequences', 'train_labels', 'test_sequences', 'test_labels'\n",
        "# Representing input sequences and labels for the training and testing sets\n",
        "\n",
        "# LSTM Model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(x)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# Hyperparameters\n",
        "vocab_size = len(index_to_word)  # Assuming vocabulary size is 10000\n",
        "embedding_dim = 128  # Embedding dimension\n",
        "hidden_dim = 256  # LSTM hidden layer dimension\n",
        "output_dim = vocab_size  # Output dimension same as vocabulary size\n",
        "sequence_length = 10  # Set the length of each sequence\n",
        "\n",
        "# Instantiate the model\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "train_sequences = torch.tensor(train_sequences, dtype=torch.long)\n",
        "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
        "\n",
        "test_sequences = torch.tensor(test_sequences, dtype=torch.long)\n",
        "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 2048\n",
        "train_dataset = TensorDataset(train_sequences, train_labels)\n",
        "train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(test_sequences, test_labels)\n",
        "test_data_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Training and validation\n",
        "num_epochs = 3\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "total_iterations = 0  # Iteration counter\n",
        "for epoch in range(num_epochs):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for batch_inputs, batch_labels in tqdm(train_data_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - Training', position=0):\n",
        "        total_iterations += 1\n",
        "        batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_inputs)\n",
        "        loss = loss_function(outputs, batch_labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Output training loss and perplexity every 100 iterations\n",
        "        if total_iterations % 100 == 0:\n",
        "            avg_loss = total_loss / 100\n",
        "            perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "            tqdm.write(f'Iteration {total_iterations}, Training Loss: {avg_loss:.4f}, Perplexity: {perplexity:.4f}')\n",
        "            total_loss = 0.0\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for batch_inputs, batch_labels in tqdm(test_data_loader, desc=f'Epoch {epoch + 1}/{num_epochs} - Validation'):\n",
        "            batch_inputs, batch_labels = batch_inputs.to(device), batch_labels.to(device)\n",
        "            outputs = model(batch_inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_correct += (predicted == batch_labels).sum().item()\n",
        "            total_samples += batch_labels.size(0)\n",
        "\n",
        "        accuracy = total_correct / total_samples\n",
        "        tqdm.write(f'Validation Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "print('Training complete.')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRgwt25_KiHq",
        "outputId": "584dfd9a-c4df-4c40-9558-02ee0e31ad97"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-bc6874187399>:39: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_sequences = torch.tensor(train_sequences, dtype=torch.long)\n",
            "<ipython-input-7-bc6874187399>:40: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
            "<ipython-input-7-bc6874187399>:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_sequences = torch.tensor(test_sequences, dtype=torch.long)\n",
            "<ipython-input-7-bc6874187399>:43: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
            "Epoch 1/3 - Training:   3%|▎         | 102/3625 [00:06<03:04, 19.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 100, Training Loss: 7.5969, Perplexity: 1991.9426\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:   6%|▌         | 202/3625 [00:12<02:59, 19.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 200, Training Loss: 6.6152, Perplexity: 746.3846\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:   8%|▊         | 303/3625 [00:18<05:09, 10.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 300, Training Loss: 6.4147, Perplexity: 610.7785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  11%|█         | 403/3625 [00:26<03:00, 17.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 400, Training Loss: 6.2830, Perplexity: 535.3988\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  14%|█▍        | 502/3625 [00:34<07:04,  7.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 500, Training Loss: 6.1567, Perplexity: 471.8893\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  17%|█▋        | 602/3625 [00:42<02:54, 17.36it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 600, Training Loss: 6.0824, Perplexity: 438.0866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  19%|█▉        | 703/3625 [00:51<08:04,  6.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 700, Training Loss: 6.0357, Perplexity: 418.0971\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  22%|██▏       | 801/3625 [01:00<04:16, 11.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 800, Training Loss: 5.9399, Perplexity: 379.8969\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  25%|██▍       | 903/3625 [01:09<02:26, 18.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 900, Training Loss: 5.8952, Perplexity: 363.3045\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  28%|██▊       | 1002/3625 [01:17<03:41, 11.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1000, Training Loss: 5.8417, Perplexity: 344.3690\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  30%|███       | 1103/3625 [01:25<02:14, 18.71it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1100, Training Loss: 5.7882, Perplexity: 326.4131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  33%|███▎      | 1202/3625 [01:34<03:36, 11.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1200, Training Loss: 5.7523, Perplexity: 314.9085\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  36%|███▌      | 1302/3625 [01:41<02:09, 17.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1300, Training Loss: 5.6863, Perplexity: 294.8065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  39%|███▊      | 1402/3625 [01:50<03:24, 10.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1400, Training Loss: 5.6355, Perplexity: 280.2106\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  41%|████▏     | 1502/3625 [01:58<02:41, 13.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1500, Training Loss: 5.5641, Perplexity: 260.8835\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  44%|████▍     | 1600/3625 [02:06<01:50, 18.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1600, Training Loss: 5.5339, Perplexity: 253.1357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  47%|████▋     | 1702/3625 [02:15<01:53, 16.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1700, Training Loss: 5.4713, Perplexity: 237.7599\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  50%|████▉     | 1801/3625 [02:22<01:40, 18.15it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1800, Training Loss: 5.4179, Perplexity: 225.3998\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  52%|█████▏    | 1903/3625 [02:31<01:52, 15.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1900, Training Loss: 5.3633, Perplexity: 213.4359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  55%|█████▌    | 2003/3625 [02:39<01:49, 14.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2000, Training Loss: 5.3220, Perplexity: 204.7865\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  58%|█████▊    | 2103/3625 [02:48<01:48, 13.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2100, Training Loss: 5.2738, Perplexity: 195.1519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  61%|██████    | 2202/3625 [02:56<01:20, 17.69it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2200, Training Loss: 5.2252, Perplexity: 185.8950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  64%|██████▎   | 2303/3625 [03:04<02:00, 10.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2300, Training Loss: 5.1595, Perplexity: 174.0737\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  66%|██████▋   | 2403/3625 [03:12<01:07, 18.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2400, Training Loss: 5.1105, Perplexity: 165.7594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  69%|██████▉   | 2501/3625 [03:21<03:11,  5.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2500, Training Loss: 5.0671, Perplexity: 158.7203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  72%|███████▏  | 2603/3625 [03:28<01:00, 16.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2600, Training Loss: 5.0159, Perplexity: 150.7880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  75%|███████▍  | 2702/3625 [03:36<00:49, 18.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2700, Training Loss: 4.9687, Perplexity: 143.8364\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  77%|███████▋  | 2803/3625 [03:45<00:52, 15.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2800, Training Loss: 4.9266, Perplexity: 137.9134\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  80%|████████  | 2903/3625 [03:53<00:39, 18.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 2900, Training Loss: 4.8686, Perplexity: 130.1435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  83%|████████▎ | 3002/3625 [04:02<01:04,  9.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3000, Training Loss: 4.8245, Perplexity: 124.5181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  86%|████████▌ | 3103/3625 [04:09<00:28, 18.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3100, Training Loss: 4.7850, Perplexity: 119.7050\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  88%|████████▊ | 3203/3625 [04:18<00:34, 12.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3200, Training Loss: 4.7538, Perplexity: 116.0269\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  91%|█████████ | 3303/3625 [04:26<00:17, 17.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3300, Training Loss: 4.6955, Perplexity: 109.4490\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  94%|█████████▍| 3403/3625 [04:35<00:23,  9.41it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3400, Training Loss: 4.6663, Perplexity: 106.3082\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  97%|█████████▋| 3502/3625 [04:42<00:08, 14.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3500, Training Loss: 4.6127, Perplexity: 100.7510\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training:  99%|█████████▉| 3603/3625 [04:51<00:02,  7.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3600, Training Loss: 4.5558, Perplexity: 95.1810\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/3 - Training: 100%|██████████| 3625/3625 [04:52<00:00, 12.37it/s]\n",
            "Epoch 1/3 - Validation: 100%|██████████| 907/907 [00:49<00:00, 18.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 29.30%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:   2%|▏         | 77/3625 [00:06<04:21, 13.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3700, Training Loss: 3.3021, Perplexity: 27.1709\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:   5%|▍         | 177/3625 [00:15<03:50, 14.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3800, Training Loss: 4.3666, Perplexity: 78.7785\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:   8%|▊         | 277/3625 [00:24<03:46, 14.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 3900, Training Loss: 4.3447, Perplexity: 77.0708\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  10%|█         | 378/3625 [00:32<04:16, 12.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4000, Training Loss: 4.2992, Perplexity: 73.6429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  13%|█▎        | 478/3625 [00:40<02:56, 17.83it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4100, Training Loss: 4.2712, Perplexity: 71.6072\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  16%|█▌        | 578/3625 [00:49<05:20,  9.51it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4200, Training Loss: 4.2431, Perplexity: 69.6264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  19%|█▊        | 678/3625 [00:57<02:50, 17.24it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4300, Training Loss: 4.2170, Perplexity: 67.8290\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  21%|██▏       | 774/3625 [01:07<04:01, 11.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4400, Training Loss: 4.1758, Perplexity: 65.0932\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  24%|██▍       | 877/3625 [01:18<03:24, 13.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4500, Training Loss: 4.1481, Perplexity: 63.3133\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  27%|██▋       | 975/3625 [01:26<02:26, 18.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4600, Training Loss: 4.1069, Perplexity: 60.7573\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  30%|██▉       | 1077/3625 [01:35<03:04, 13.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4700, Training Loss: 4.0882, Perplexity: 59.6339\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  32%|███▏      | 1178/3625 [01:42<02:11, 18.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4800, Training Loss: 4.0625, Perplexity: 58.1221\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  35%|███▌      | 1277/3625 [01:51<02:49, 13.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 4900, Training Loss: 4.0203, Perplexity: 55.7200\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  38%|███▊      | 1377/3625 [01:59<02:37, 14.28it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5000, Training Loss: 4.0053, Perplexity: 54.8904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  41%|████      | 1477/3625 [02:08<03:52,  9.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5100, Training Loss: 3.9773, Perplexity: 53.3701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  44%|████▎     | 1577/3625 [02:16<01:54, 17.86it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5200, Training Loss: 3.9356, Perplexity: 51.1919\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  46%|████▋     | 1677/3625 [02:25<03:59,  8.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5300, Training Loss: 3.9192, Perplexity: 50.3624\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  49%|████▉     | 1778/3625 [02:33<01:50, 16.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5400, Training Loss: 3.8968, Perplexity: 49.2435\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  52%|█████▏    | 1876/3625 [02:41<01:57, 14.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5500, Training Loss: 3.8764, Perplexity: 48.2519\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  55%|█████▍    | 1977/3625 [02:50<01:44, 15.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5600, Training Loss: 3.8398, Perplexity: 46.5182\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  57%|█████▋    | 2077/3625 [02:58<01:24, 18.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5700, Training Loss: 3.8269, Perplexity: 45.9208\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  60%|██████    | 2177/3625 [03:06<01:57, 12.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5800, Training Loss: 3.7819, Perplexity: 43.9007\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  63%|██████▎   | 2277/3625 [03:14<01:13, 18.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 5900, Training Loss: 3.7580, Perplexity: 42.8634\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  66%|██████▌   | 2378/3625 [03:23<02:06,  9.82it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6000, Training Loss: 3.7326, Perplexity: 41.7866\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  68%|██████▊   | 2478/3625 [03:30<01:04, 17.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6100, Training Loss: 3.7157, Perplexity: 41.0881\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  71%|███████   | 2578/3625 [03:40<01:52,  9.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6200, Training Loss: 3.6917, Perplexity: 40.1150\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  74%|███████▍  | 2678/3625 [03:47<00:54, 17.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6300, Training Loss: 3.6721, Perplexity: 39.3346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  77%|███████▋  | 2778/3625 [03:56<01:50,  7.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6400, Training Loss: 3.6490, Perplexity: 38.4359\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  79%|███████▉  | 2878/3625 [04:04<00:59, 12.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6500, Training Loss: 3.6207, Perplexity: 37.3635\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  82%|████████▏ | 2978/3625 [04:11<00:34, 18.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6600, Training Loss: 3.6038, Perplexity: 36.7383\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  85%|████████▍ | 3078/3625 [04:20<00:37, 14.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6700, Training Loss: 3.5894, Perplexity: 36.2125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  88%|████████▊ | 3178/3625 [04:28<00:24, 18.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6800, Training Loss: 3.5556, Perplexity: 35.0099\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  90%|█████████ | 3278/3625 [04:37<00:27, 12.68it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 6900, Training Loss: 3.5242, Perplexity: 33.9264\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  93%|█████████▎| 3378/3625 [04:45<00:17, 14.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7000, Training Loss: 3.5091, Perplexity: 33.4174\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  96%|█████████▌| 3478/3625 [04:54<00:14, 10.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7100, Training Loss: 3.4865, Perplexity: 32.6716\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training:  99%|█████████▊| 3578/3625 [05:03<00:02, 17.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7200, Training Loss: 3.4686, Perplexity: 32.0918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/3 - Training: 100%|██████████| 3625/3625 [05:07<00:00, 11.79it/s]\n",
            "Epoch 2/3 - Validation: 100%|██████████| 907/907 [00:48<00:00, 18.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 41.07%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:   1%|▏         | 52/3625 [00:04<04:16, 13.91it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7300, Training Loss: 1.6378, Perplexity: 5.1439\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:   4%|▍         | 150/3625 [00:12<03:19, 17.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7400, Training Loss: 3.2711, Perplexity: 26.3397\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:   7%|▋         | 252/3625 [00:20<04:02, 13.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7500, Training Loss: 3.2559, Perplexity: 25.9442\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  10%|▉         | 352/3625 [00:29<02:58, 18.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7600, Training Loss: 3.2362, Perplexity: 25.4358\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  12%|█▏        | 452/3625 [00:37<04:40, 11.31it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7700, Training Loss: 3.2225, Perplexity: 25.0909\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  15%|█▌        | 552/3625 [00:45<02:54, 17.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7800, Training Loss: 3.2231, Perplexity: 25.1049\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  18%|█▊        | 653/3625 [00:54<04:35, 10.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 7900, Training Loss: 3.1961, Perplexity: 24.4377\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  21%|██        | 752/3625 [01:02<02:43, 17.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8000, Training Loss: 3.1928, Perplexity: 24.3574\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  24%|██▎       | 852/3625 [01:11<04:59,  9.25it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8100, Training Loss: 3.1701, Perplexity: 23.8103\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  26%|██▋       | 952/3625 [01:18<03:29, 12.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8200, Training Loss: 3.1493, Perplexity: 23.3189\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  29%|██▉       | 1052/3625 [01:26<02:18, 18.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8300, Training Loss: 3.1443, Perplexity: 23.2024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  32%|███▏      | 1152/3625 [01:35<02:41, 15.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8400, Training Loss: 3.1253, Perplexity: 22.7667\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  35%|███▍      | 1252/3625 [01:43<02:10, 18.17it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8500, Training Loss: 3.1243, Perplexity: 22.7434\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  37%|███▋      | 1352/3625 [01:52<03:02, 12.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8600, Training Loss: 3.1083, Perplexity: 22.3833\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  40%|████      | 1452/3625 [02:00<02:29, 14.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8700, Training Loss: 3.1003, Perplexity: 22.2054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  43%|████▎     | 1553/3625 [02:08<02:46, 12.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8800, Training Loss: 3.0754, Perplexity: 21.6593\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  46%|████▌     | 1653/3625 [02:17<01:51, 17.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 8900, Training Loss: 3.0568, Perplexity: 21.2592\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  48%|████▊     | 1752/3625 [02:25<04:04,  7.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9000, Training Loss: 3.0531, Perplexity: 21.1802\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  51%|█████     | 1853/3625 [02:33<01:42, 17.27it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9100, Training Loss: 3.0316, Perplexity: 20.7302\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  54%|█████▍    | 1951/3625 [02:42<06:01,  4.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9200, Training Loss: 3.0308, Perplexity: 20.7130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  57%|█████▋    | 2053/3625 [02:50<01:38, 15.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9300, Training Loss: 3.0062, Perplexity: 20.2111\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  59%|█████▉    | 2153/3625 [02:58<01:20, 18.40it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9400, Training Loss: 3.0005, Perplexity: 20.0959\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  62%|██████▏   | 2253/3625 [03:06<01:32, 14.89it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9500, Training Loss: 2.9816, Perplexity: 19.7190\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  65%|██████▍   | 2353/3625 [03:15<01:12, 17.58it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9600, Training Loss: 2.9699, Perplexity: 19.4897\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  68%|██████▊   | 2453/3625 [03:23<01:55, 10.12it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9700, Training Loss: 2.9500, Perplexity: 19.1063\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  70%|███████   | 2553/3625 [03:31<01:01, 17.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9800, Training Loss: 2.9364, Perplexity: 18.8478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  73%|███████▎  | 2651/3625 [03:41<02:14,  7.23it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 9900, Training Loss: 2.9329, Perplexity: 18.7826\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  76%|███████▌  | 2753/3625 [03:49<00:49, 17.45it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10000, Training Loss: 2.9261, Perplexity: 18.6541\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  79%|███████▊  | 2849/3625 [03:57<00:42, 18.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10100, Training Loss: 2.9066, Perplexity: 18.2942\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  81%|████████▏ | 2953/3625 [04:06<00:49, 13.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10200, Training Loss: 2.8800, Perplexity: 17.8141\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  84%|████████▍ | 3051/3625 [04:13<00:32, 17.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10300, Training Loss: 2.8831, Perplexity: 17.8701\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  87%|████████▋ | 3153/3625 [04:22<00:31, 14.87it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10400, Training Loss: 2.8606, Perplexity: 17.4720\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  90%|████████▉ | 3253/3625 [04:30<00:20, 18.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10500, Training Loss: 2.8573, Perplexity: 17.4136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  92%|█████████▏| 3353/3625 [04:39<00:19, 13.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10600, Training Loss: 2.8409, Perplexity: 17.1306\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  95%|█████████▌| 3451/3625 [04:47<00:12, 13.70it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10700, Training Loss: 2.8229, Perplexity: 16.8255\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training:  98%|█████████▊| 3553/3625 [04:55<00:06, 10.67it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10800, Training Loss: 2.8127, Perplexity: 16.6546\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/3 - Training: 100%|██████████| 3625/3625 [05:02<00:00, 11.99it/s]\n",
            "Epoch 3/3 - Validation: 100%|██████████| 907/907 [00:48<00:00, 18.62it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 49.74%\n",
            "Training complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(vocab)"
      ],
      "metadata": {
        "id": "MB2cVDGfVOkR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zSm5rEpsVl5J",
        "outputId": "b03cdaec-a3fc-4f13-a847-5a4343f111c7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'下论'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'model.pth')"
      ],
      "metadata": {
        "id": "hXSBphekeiTZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import random\n",
        "from torch.nn import functional as F\n",
        "\n",
        "model.eval()\n",
        "\n",
        "org = [\n",
        "    \"唉哟 P.Q我来自上海的兄弟\",\n",
        "    \"What’s up\",\n",
        "     \"我的新疆homie\",\n",
        "     \"Are you ready\",\n",
        "     \"I’m ready\",\n",
        "    \"Let’s go\",\n",
        "    \"初次去上海只为见心爱的她\",\n",
        "    \"南京路上相拥\",\n",
        "    \"听跨年钟声嘀嗒\",\n",
        "    \"忍不住狂甩对方嘴唇在那一刹\",\n",
        "    \"更幸福的是\",\n",
        "    \"现在看我们孩子长大\",\n",
        "    \"街上都是迪丽热巴和古力娜扎\",\n",
        "    \"在乌鲁木齐每一天的太阳\",\n",
        "    \"So hot\",\n",
        "    \"忘不了那抓饭的香和大盘鸡的辣\",\n",
        "     \"我喝了十六次的石榴汁在大巴扎\",\n",
        "     \"那年上海夺了冠\",\n",
        "     \"狗哥微博发过赞\",\n",
        "     \"P.Q带我吃的串\",\n",
        "    \"象哥的酒量赢一半\"\n",
        "]\n",
        "\n",
        "# Function to convert generated indices back to words\n",
        "def index_to_word(index, vocab):\n",
        "    return vocab[index]\n",
        "\n",
        "# Function to generate sentences\n",
        "def generate_sentence(model, initial_word, vocab, max_length=5):\n",
        "    model.eval()\n",
        "    # model.to('cpu')\n",
        "    # Convert the initial word to an index\n",
        "    current_word = torch.tensor([word_to_index[initial_word]], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    generated_sentence = [initial_word]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            output = model(current_word)\n",
        "            probabilities = F.softmax(output, dim=1).squeeze(0)\n",
        "            predicted_index = torch.multinomial(probabilities, 1).item()\n",
        "            predicted_word = index_to_word(predicted_index, vocab)\n",
        "\n",
        "            if 'EOS' in predicted_word:\n",
        "                break\n",
        "\n",
        "            generated_sentence.append(predicted_word)\n",
        "            current_word = torch.tensor([predicted_index], dtype=torch.long).unsqueeze(0).to(device)\n",
        "\n",
        "    return ' '.join(generated_sentence)\n",
        "\n",
        "# Test sentence generation\n",
        "generated_sentences = []\n",
        "for org_sentence in org:\n",
        "    initial_word = org_sentence[0]  # Use the first character of each sentence as the initial word\n",
        "    generated_sentence = generate_sentence(model, initial_word, vocab)\n",
        "    generated_sentences.append(generated_sentence)\n",
        "\n",
        "# Print the generated sentences\n",
        "for org_sentence, gen_sentence in zip(org, generated_sentences):\n",
        "    print(f\"Original: {org_sentence}\")\n",
        "    print(f\"Generated: {gen_sentence}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LY1ouQJ4Kefp",
        "outputId": "1bede291-25c5-4e8b-d0f1-0d4aed6fe2c2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: 唉哟 P.Q我来自上海的兄弟\n",
            "Generated: 唉 千奇百怪 相处 阴晴圆 浓郁 Whiskey\n",
            "--------------------------------------------------\n",
            "Original: What’s up\n",
            "Generated: W koralmaysan\n",
            "--------------------------------------------------\n",
            "Original: 我的新疆homie\n",
            "Generated: 我 握紧 卡卡 先人 四合\n",
            "--------------------------------------------------\n",
            "Original: Are you ready\n",
            "Generated: A 喷气式 我梦里 思绪 太过火\n",
            "--------------------------------------------------\n",
            "Original: I’m ready\n",
            "Generated: I 胡言\n",
            "--------------------------------------------------\n",
            "Original: Let’s go\n",
            "Generated: L 笨重\n",
            "--------------------------------------------------\n",
            "Original: 初次去上海只为见心爱的她\n",
            "Generated: 初 脑洞 鲜 静静的 Do GogoBoi\n",
            "--------------------------------------------------\n",
            "Original: 南京路上相拥\n",
            "Generated: 南 走險 里路 淫荡 照料 视频\n",
            "--------------------------------------------------\n",
            "Original: 听跨年钟声嘀嗒\n",
            "Generated: 听 牺牲品 走私犯 fendi 盘膝 全班\n",
            "--------------------------------------------------\n",
            "Original: 忍不住狂甩对方嘴唇在那一刹\n",
            "Generated: 忍 rolling 摧残 DIZZY 除病 不择手段\n",
            "--------------------------------------------------\n",
            "Original: 更幸福的是\n",
            "Generated: 更 Judge 放电影 宠物\n",
            "--------------------------------------------------\n",
            "Original: 现在看我们孩子长大\n",
            "Generated: 现 fighting 开出 微醉 烟味 灰烬\n",
            "--------------------------------------------------\n",
            "Original: 街上都是迪丽热巴和古力娜扎\n",
            "Generated: 街 后街\n",
            "--------------------------------------------------\n",
            "Original: 在乌鲁木齐每一天的太阳\n",
            "Generated: 在 疾风 资讯\n",
            "--------------------------------------------------\n",
            "Original: So hot\n",
            "Generated: S 老师傅 忿 than 来晚 upright\n",
            "--------------------------------------------------\n",
            "Original: 忘不了那抓饭的香和大盘鸡的辣\n",
            "Generated: 忘 暴风 冲前 UP2U 周 背弃\n",
            "--------------------------------------------------\n",
            "Original: 我喝了十六次的石榴汁在大巴扎\n",
            "Generated: 我 下酒菜 好几代 预热\n",
            "--------------------------------------------------\n",
            "Original: 那年上海夺了冠\n",
            "Generated: 那 kk dubs 磕 心血 喉舌\n",
            "--------------------------------------------------\n",
            "Original: 狗哥微博发过赞\n",
            "Generated: 狗\n",
            "--------------------------------------------------\n",
            "Original: P.Q带我吃的串\n",
            "Generated: P 上闭 弘扬 stuff 里学 real\n",
            "--------------------------------------------------\n",
            "Original: 象哥的酒量赢一半\n",
            "Generated: 象\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "# Tokenize the original and generated sentences\n",
        "tokenized_org = [nltk.word_tokenize(sentence.lower()) for sentence in org]\n",
        "tokenized_gen = [nltk.word_tokenize(sentence.lower()) for sentence in generated_sentences]\n",
        "\n",
        "# Calculate BLEU score with lower n-gram order (e.g., 1-gram)\n",
        "bleu_scores = [sentence_bleu([tokens], generated_tokens, weights=(4, 0, 0, 0)) for tokens, generated_tokens in zip(tokenized_org, tokenized_gen)]\n",
        "\n",
        "# Print BLEU scores\n",
        "for org_sentence, gen_sentence, bleu_score in zip(org, generated_sentences, bleu_scores):\n",
        "    print(f\"Original: {org_sentence}\")\n",
        "    print(f\"Generated: {gen_sentence}\")\n",
        "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyP9bOOvhDZq",
        "outputId": "34c06bd7-377e-4e13-ee5d-070f03621514"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: 唉哟 P.Q我来自上海的兄弟\n",
            "Generated: 唉 千奇百怪 相处 阴晴圆 浓郁 Whiskey\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: What’s up\n",
            "Generated: W koralmaysan\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 我的新疆homie\n",
            "Generated: 我 握紧 卡卡 先人 四合\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: Are you ready\n",
            "Generated: A 喷气式 我梦里 思绪 太过火\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: I’m ready\n",
            "Generated: I 胡言\n",
            "BLEU Score: 0.0230\n",
            "--------------------------------------------------\n",
            "Original: Let’s go\n",
            "Generated: L 笨重\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 初次去上海只为见心爱的她\n",
            "Generated: 初 脑洞 鲜 静静的 Do GogoBoi\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 南京路上相拥\n",
            "Generated: 南 走險 里路 淫荡 照料 视频\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 听跨年钟声嘀嗒\n",
            "Generated: 听 牺牲品 走私犯 fendi 盘膝 全班\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 忍不住狂甩对方嘴唇在那一刹\n",
            "Generated: 忍 rolling 摧残 DIZZY 除病 不择手段\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 更幸福的是\n",
            "Generated: 更 Judge 放电影 宠物\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 现在看我们孩子长大\n",
            "Generated: 现 fighting 开出 微醉 烟味 灰烬\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 街上都是迪丽热巴和古力娜扎\n",
            "Generated: 街 后街\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 在乌鲁木齐每一天的太阳\n",
            "Generated: 在 疾风 资讯\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: So hot\n",
            "Generated: S 老师傅 忿 than 来晚 upright\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 忘不了那抓饭的香和大盘鸡的辣\n",
            "Generated: 忘 暴风 冲前 UP2U 周 背弃\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 我喝了十六次的石榴汁在大巴扎\n",
            "Generated: 我 下酒菜 好几代 预热\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 那年上海夺了冠\n",
            "Generated: 那 kk dubs 磕 心血 喉舌\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 狗哥微博发过赞\n",
            "Generated: 狗\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: P.Q带我吃的串\n",
            "Generated: P 上闭 弘扬 stuff 里学 real\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n",
            "Original: 象哥的酒量赢一半\n",
            "Generated: 象\n",
            "BLEU Score: 0.0000\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_org"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nx8Mjb5VkXxP",
        "outputId": "11195bae-179b-4b96-c5ef-81038d62a4a9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['唉哟', 'p.q我来自上海的兄弟'],\n",
              " ['what', '’', 's', 'up'],\n",
              " ['我的新疆homie'],\n",
              " ['are', 'you', 'ready'],\n",
              " ['i', '’', 'm', 'ready'],\n",
              " ['let', '’', 's', 'go'],\n",
              " ['初次去上海只为见心爱的她'],\n",
              " ['南京路上相拥'],\n",
              " ['听跨年钟声嘀嗒'],\n",
              " ['忍不住狂甩对方嘴唇在那一刹'],\n",
              " ['更幸福的是'],\n",
              " ['现在看我们孩子长大'],\n",
              " ['街上都是迪丽热巴和古力娜扎'],\n",
              " ['在乌鲁木齐每一天的太阳'],\n",
              " ['so', 'hot'],\n",
              " ['忘不了那抓饭的香和大盘鸡的辣'],\n",
              " ['我喝了十六次的石榴汁在大巴扎'],\n",
              " ['那年上海夺了冠'],\n",
              " ['狗哥微博发过赞'],\n",
              " ['p.q带我吃的串'],\n",
              " ['象哥的酒量赢一半']]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I8ttwVeqKPET"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PRY8U4kJKKwJ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:47:39.296447Z",
          "iopub.execute_input": "2023-12-11T17:47:39.296859Z",
          "iopub.status.idle": "2023-12-11T17:48:14.204664Z",
          "shell.execute_reply.started": "2023-12-11T17:47:39.296816Z",
          "shell.execute_reply": "2023-12-11T17:48:14.203826Z"
        },
        "trusted": true,
        "id": "DPWZWUvtBPSu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T14:01:43.041246Z",
          "iopub.execute_input": "2023-12-11T14:01:43.041888Z",
          "iopub.status.idle": "2023-12-11T14:01:43.046083Z",
          "shell.execute_reply.started": "2023-12-11T14:01:43.041854Z",
          "shell.execute_reply": "2023-12-11T14:01:43.045033Z"
        },
        "trusted": true,
        "id": "PrIDaPtSBPSu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:48:14.206405Z",
          "iopub.execute_input": "2023-12-11T17:48:14.206678Z",
          "iopub.status.idle": "2023-12-11T17:48:19.971722Z",
          "shell.execute_reply.started": "2023-12-11T17:48:14.206655Z",
          "shell.execute_reply": "2023-12-11T17:48:19.970672Z"
        },
        "trusted": true,
        "id": "8vjatIlBBPSu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:48:19.972940Z",
          "iopub.execute_input": "2023-12-11T17:48:19.973304Z",
          "iopub.status.idle": "2023-12-11T17:48:19.977693Z",
          "shell.execute_reply.started": "2023-12-11T17:48:19.973271Z",
          "shell.execute_reply": "2023-12-11T17:48:19.976801Z"
        },
        "trusted": true,
        "id": "qsPMlZfUBPSv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:48:19.980308Z",
          "iopub.execute_input": "2023-12-11T17:48:19.980928Z",
          "iopub.status.idle": "2023-12-11T17:48:53.062333Z",
          "shell.execute_reply.started": "2023-12-11T17:48:19.980895Z",
          "shell.execute_reply": "2023-12-11T17:48:53.061455Z"
        },
        "trusted": true,
        "id": "OpCM4CtlBPSv"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 假设 X_padded 和 y 是你的完整数据集\n",
        "# X_padded: 输入序列\n",
        "# y: 目标标签\n",
        "\n",
        "# 设置切分比例，例如 80% 训练数据，20% 验证数据\n",
        "train_size = 0.8\n",
        "\n",
        "# 切分数据\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_padded, y, train_size=train_size, random_state=42)\n",
        "\n",
        "# 打印切分后数据的大小\n",
        "print(\"训练集大小:\", X_train.shape)\n",
        "print(\"验证集大小:\", X_val.shape)\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:48:53.063563Z",
          "iopub.execute_input": "2023-12-11T17:48:53.063856Z",
          "iopub.status.idle": "2023-12-11T17:48:54.582179Z",
          "shell.execute_reply.started": "2023-12-11T17:48:53.063830Z",
          "shell.execute_reply": "2023-12-11T17:48:54.581210Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "XHXdjVcCBPSv",
        "outputId": "2dfe8de2-7b0f-447e-fb40-67653cdb5cf2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-6660fd4cba98>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# 切分数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_padded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 打印切分后数据的大小\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'X_padded' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "class LyricsDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "# LSTM 模型\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(x)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:48:54.583424Z",
          "iopub.execute_input": "2023-12-11T17:48:54.583718Z",
          "iopub.status.idle": "2023-12-11T17:48:54.592866Z",
          "shell.execute_reply.started": "2023-12-11T17:48:54.583693Z",
          "shell.execute_reply": "2023-12-11T17:48:54.591796Z"
        },
        "trusted": true,
        "id": "TwQyNgiLBPSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(word_to_index)\n",
        "embedding_dim = 512  # 嵌入维度\n",
        "hidden_dim = 256  # LSTM 隐藏层维度\n",
        "output_dim = vocab_size  # 输出维度与词汇表大小相同"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:48:54.593949Z",
          "iopub.execute_input": "2023-12-11T17:48:54.594251Z",
          "iopub.status.idle": "2023-12-11T17:48:54.606893Z",
          "shell.execute_reply.started": "2023-12-11T17:48:54.594220Z",
          "shell.execute_reply": "2023-12-11T17:48:54.606092Z"
        },
        "trusted": true,
        "id": "xr46PEMqBPSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:48:54.607972Z",
          "iopub.execute_input": "2023-12-11T17:48:54.608280Z",
          "iopub.status.idle": "2023-12-11T17:48:54.620569Z",
          "shell.execute_reply.started": "2023-12-11T17:48:54.608248Z",
          "shell.execute_reply": "2023-12-11T17:48:54.619558Z"
        },
        "trusted": true,
        "id": "CuUy-eZ6BPSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 准备数据加载器\n",
        "batch_size = 1024\n",
        "# 准备数据加载器\n",
        "train_dataset = LyricsDataset(X_train, y_train)\n",
        "val_dataset = LyricsDataset(X_val, y_val)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:48:54.621813Z",
          "iopub.execute_input": "2023-12-11T17:48:54.622131Z",
          "iopub.status.idle": "2023-12-11T17:48:54.629604Z",
          "shell.execute_reply.started": "2023-12-11T17:48:54.622106Z",
          "shell.execute_reply": "2023-12-11T17:48:54.628843Z"
        },
        "trusted": true,
        "id": "jHNDD2NMBPSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# 实例化模型\n",
        "model = LSTMModel(vocab_size+1, embedding_dim, hidden_dim, output_dim)\n",
        "model.to(device)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 10\n",
        "\n",
        "# 训练和评估模型\n",
        "def evaluate_model(model, data_loader, loss_function):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in data_loader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(data_loader)\n",
        "\n",
        "# from tqdm import tqdm\n",
        "\n",
        "# 在训练循环外部初始化 tqdm 进度条\n",
        "# epoch_bar = tqdm(range(num_epochs), desc=\"Epochs\")\n",
        "\n",
        "from tqdm import tqdm\n",
        "# epoch_bar = tqdm(range(num_epochs), total=num_epochs,desc=\"Epochs\", position=0)\n",
        "# 在训练循环外部初始化 tqdm 进度条\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "\n",
        "    # 在 train_loader 外部初始化 tqdm 进度条\n",
        "    train_iterator = tqdm(enumerate(train_loader), total=len(train_loader), desc=\"Training Iterations\", position=0)\n",
        "\n",
        "    for i,(inputs, targets) in train_iterator:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 10 == 0:\n",
        "            # 打印信息的部分保留，如果需要的话\n",
        "            train_iterator.set_postfix({'desc':f\"Epoch {epoch+1}, Iteration {i}, Train Loss: {loss},  Perplexity: {np.exp(loss.item())}\"})\n",
        "\n",
        "    # 关闭 train_iterator 进度条\n",
        "#     train_iterator.close()\n",
        "\n",
        "    # 在验证集上评估模型\n",
        "    # train_loss = evaluate_model(model, train_loader, loss_function)\n",
        "    val_loss = evaluate_model(model, val_loader, loss_function)\n",
        "\n",
        "    # 打印信息的部分保留，如果需要的话\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Val Loss: {val_loss} ,Perplexity: {np.exp(val_loss)}\")"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:54:01.709813Z",
          "iopub.execute_input": "2023-12-11T17:54:01.710220Z"
        },
        "trusted": true,
        "id": "fNqug8RLBPSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tqdm\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-11T17:34:27.408438Z",
          "iopub.execute_input": "2023-12-11T17:34:27.408816Z",
          "iopub.status.idle": "2023-12-11T17:34:39.972861Z",
          "shell.execute_reply.started": "2023-12-11T17:34:27.408788Z",
          "shell.execute_reply": "2023-12-11T17:34:39.971842Z"
        },
        "trusted": true,
        "id": "dOQ5_LJvBPSw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rPqBBATHBPSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xOhHOhaXBPSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cefH49iFBPSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text[:30]"
      ],
      "metadata": {
        "id": "blpKlKMoBPSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LSTM"
      ],
      "metadata": {
        "id": "vOhQCBClBPSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jieba\n",
        "import torch\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        text = file.read()\n",
        "    return text\n",
        "\n",
        "def preprocess_text(text):\n",
        "    words = list(jieba.cut(text))\n",
        "    return words\n",
        "\n",
        "def create_vocab(words):\n",
        "    word_counts = Counter(words)\n",
        "    vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
        "    word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
        "    idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
        "    return word_to_idx, idx_to_word\n",
        "\n",
        "def create_sequences(words, word_to_idx, seq_length=20):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    for i in range(len(words) - seq_length):\n",
        "        seq = words[i:i+seq_length]\n",
        "        label = words[i+seq_length]\n",
        "        sequences.append([word_to_idx[word] for word in seq])\n",
        "        labels.append(word_to_idx[label])\n",
        "    return sequences, labels\n",
        "\n",
        "# 读取和预处理文本\n",
        "file_path = 'word.txt'  # 替换为你的文件路径\n",
        "text = read_file(file_path)\n",
        "words = preprocess_text(text)\n",
        "\n",
        "# 创建词汇表和索引\n",
        "word_to_idx, idx_to_word = create_vocab(words)\n",
        "\n",
        "# 创建序列和标签\n",
        "seq_length = 20  # 可根据需要调整序列长度\n",
        "sequences, labels = create_sequences(words, word_to_idx, seq_length)\n",
        "\n",
        "# 将序列和标签转换为 PyTorch 张量\n",
        "sequences = torch.tensor(sequences, dtype=torch.long)\n",
        "labels = torch.tensor(labels, dtype=torch.long)\n"
      ],
      "metadata": {
        "id": "YXx09lptBPSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels,sequences"
      ],
      "metadata": {
        "id": "nyPoXPSOBPSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(idx_to_word)\n",
        "print(\"Vocabulary size:\", vocab_size)\n"
      ],
      "metadata": {
        "id": "FcQ9yKrxBPSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "# 假设您已经有了处理过的数据：'sequences' 和 'labels'\n",
        "# sequences: 输入序列的集合\n",
        "# labels: 每个序列的下一个词的标签\n",
        "\n",
        "class LyricsDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.sequences[idx], self.labels[idx]\n",
        "\n",
        "# LSTM 模型\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        output, (hidden, cell) = self.lstm(x)\n",
        "        output = self.fc(output[:, -1, :])\n",
        "        return output\n",
        "\n",
        "# 设置超参数\n",
        "# vocab_size = vocab_size  # 假设词汇表大小为 10000\n",
        "embedding_dim = 128  # 嵌入维度\n",
        "hidden_dim = 256  # LSTM 隐藏层维度\n",
        "output_dim = vocab_size  # 输出维度与词汇表大小相同\n",
        "\n",
        "# 实例化模型\n",
        "model = LSTMModel(vocab_size, embedding_dim, hidden_dim, output_dim)\n",
        "\n",
        "# 定义损失函数和优化器\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 准备数据加载器\n",
        "batch_size = 64\n",
        "dataset = LyricsDataset(sequences, labels)\n",
        "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# 训练模型\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    for inputs, targets in data_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n"
      ],
      "metadata": {
        "id": "kuL-wp9kBPSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aa4xexosBPSy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}